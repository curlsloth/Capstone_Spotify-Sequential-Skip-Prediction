{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e66449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import io\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open('../data/raw/20181120_track_features.tar.gz', 'r:gz')\n",
    "csv_files = tar.getnames()\n",
    "\n",
    "tf_df_list = []\n",
    "\n",
    "for csv_file in [csv_files[2], csv_files[4]]:\n",
    "    csv_contents = tar.extractfile(csv_file).read()\n",
    "    tf_df_list.append(pd.read_csv(io.BytesIO(csv_contents), encoding='utf8'))\n",
    "\n",
    "tf_df = pd.concat(tf_df_list, ignore_index=True)\n",
    "tf_df.rename(columns={'track_id':'track_id_clean'}, inplace=True)\n",
    "\n",
    "kmean300_df = pd.read_csv('../data/interim/all_data/mbKMeans300clusters.csv', usecols=['track_id','clus'])\n",
    "kmean300_df.rename(columns={'track_id':'track_id_clean'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff380d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(df_hist, df_lookup, sim_file_list, score_name_list):\n",
    "    df_hist['ListenYes'] = (df_hist['skip_2'] == False)*1\n",
    "    df_hist['ListenYes'].replace(0, -1, inplace = True)\n",
    "    df_hist = df_hist.groupby(['session_id', 'clus']).agg({'ListenYes':['sum']})\n",
    "    df_hist = df_hist.reset_index()\n",
    "    df_hist.columns = df_hist.columns.droplevel(level = 1) # take out the unwanted level\n",
    "    df_pivot = pd.pivot_table(df_hist, values = 'ListenYes',index='session_id', columns='clus')\n",
    "    df_pivot = df_pivot.fillna(0)\n",
    "    \n",
    "    \n",
    "    for sim_file, score_name in zip(sim_file_list, score_name_list):\n",
    "        sim_matrix = pd.read_csv(sim_file).drop(columns=['Unnamed: 0'])\n",
    "        sim_matrix.columns = list(map(str, range(0,len(sim_matrix))))\n",
    "        df_sim_session = df_pivot.dot(sim_matrix)/sim_matrix.sum()\n",
    "        \n",
    "        df_lookup[score_name] = df_sim_session.lookup(df_lookup['session_id'],df_lookup['clus'].astype(str))\n",
    "    \n",
    "    return df_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc506af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "for n in range(9):\n",
    "    file_list = file_list + glob.glob('../data/raw/training_set/log_'+str(n)+'*.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbde35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for nShuffle in range(10):\n",
    "    df_lookup_list = []\n",
    "    random.shuffle(file_list) # randomly sampled 30 files to do parameter tuning\n",
    "    for file in file_list[0:30]:\n",
    "        log_df = pd.read_csv(file)\n",
    "\n",
    "        log_df = log_df[['session_id','track_id_clean','skip_2','session_position','session_length','hour_of_day','premium']].merge(kmean300_df)\n",
    "\n",
    "        log_df_1 = log_df.loc[log_df['session_position']<(log_df['session_length']/2)]\n",
    "\n",
    "        # as the entire dataset will be too big to train, \n",
    "        # train the first track of the 2nd half (prediction set) should be enough, \n",
    "        # as this track contribute most to the spotify metric\n",
    "\n",
    "        half_cut = log_df['session_length']/2\n",
    "\n",
    "        log_df_2 = log_df.loc[(log_df['session_position']>=half_cut) & (log_df['session_position']<half_cut+1)]\n",
    "        log_df_2['weight'] = 1\n",
    "\n",
    "\n",
    "        sim_file_list = ['../models/SVD/similarity/k300_CanbDist.csv',\n",
    "                         '../models/SVD/similarity/k300_CosSim.csv',\n",
    "                         '../models/SVD/similarity/k300_LinCorr.csv',\n",
    "                         '../models/SVD/similarity/k300_ManhDist.csv',\n",
    "                         '../models/SVD/similarity/k300_HammDist.csv',\n",
    "                         '../models/SVD/similarity/k300_SpearCorr.csv',\n",
    "                         '../models/SVD/similarity/k300_KendCorr.csv']\n",
    "        score_name_list = ['CanbDist300', 'CosSim300','LinCorr300','ManhDist300','HammDist300','SpearCorr300','KendCorr300']\n",
    "\n",
    "        df_lookup_list.append(get_sim(log_df_1, log_df_2, sim_file_list, score_name_list))\n",
    "\n",
    "\n",
    "    df_lookup = pd.concat(df_lookup_list)\n",
    "    df_lookup = df_lookup.merge(tf_df)\n",
    "    df_lookup = pd.get_dummies(df_lookup, columns=['key','time_signature','mode'])\n",
    "\n",
    "    dtrain = lgb.Dataset(df_lookup.drop(columns = ['session_id','track_id_clean','skip_2']), \n",
    "                         label=df_lookup['skip_2'],\n",
    "                         weight = df_lookup['weight'])\n",
    "\n",
    "    def bo_tune_lgb(num_leaves, learning_rate, num_iterations):\n",
    "        params = {'num_leaves': int(num_leaves),\n",
    "                  'learning_rate':learning_rate,\n",
    "                  'metric': 'binary_error',\n",
    "                  'num_iterations':int(num_iterations),\n",
    "                  'early_stopping_round':5,\n",
    "                  'objective': 'binary',\n",
    "                  'force_row_wise': True,\n",
    "                  'num_threads': 4,\n",
    "                  'verbosity': 0}\n",
    "        cv_result = lgb.cv(params, dtrain, num_boost_round=100, nfold=4)\n",
    "        return 1-cv_result['binary_error-mean'][-1]\n",
    "\n",
    "    lgb_bo = BayesianOptimization(bo_tune_lgb, {'num_leaves': (5, 40),\n",
    "                                                'learning_rate':(0,1),\n",
    "                                                'num_iterations': (10,100)\n",
    "                                                })\n",
    "\n",
    "    logger = JSONLogger(path='../models/SVD/LightGBM_BayesOpt/logs_shuffle'+str(nShuffle)+'.json')\n",
    "    lgb_bo.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "    from timeit import default_timer as timer #to see how long the computation will take\n",
    "    start = timer()\n",
    "    lgb_bo.maximize(n_iter=1, init_points=1)\n",
    "    print('Runtime: %0.2fs' % (timer() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f2792e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a470eda6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 190.79s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def bo_tune_lgb(num_leaves, learning_rate, num_iterations):\n",
    "    params = {'num_leaves': int(num_leaves),\n",
    "              'learning_rate':learning_rate,\n",
    "              'metric': 'binary_error',\n",
    "              'num_iterations':int(num_iterations),\n",
    "              'early_stopping_round':5,\n",
    "              'objective': 'binary',\n",
    "              'force_row_wise': True,\n",
    "              'num_threads': 4,\n",
    "              'verbosity': 0}\n",
    "    cv_result = lgb.cv(params, dtrain, num_boost_round=100, nfold=4)\n",
    "    return 1-cv_result['binary_error-mean'][-1]\n",
    "\n",
    "lgb_bo = BayesianOptimization(bo_tune_lgb, {'num_leaves': (5, 40),\n",
    "                                            'learning_rate':(0,1),\n",
    "                                            'num_iterations': (10,100)\n",
    "                                            })\n",
    "\n",
    "logger = JSONLogger(path=\"../models/SVD/LightGBM_BayesOpt/logs.json\")\n",
    "lgb_bo.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "from timeit import default_timer as timer #to see how long the computation will take\n",
    "start = timer()\n",
    "lgb_bo.maximize(n_iter=1, init_points=1)\n",
    "print('Runtime: %0.2fs' % (timer() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "364f2f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.6800784035870492,\n",
       " 'params': {'learning_rate': 0.573254784005274,\n",
       "  'num_iterations': 42.51568335725786,\n",
       "  'num_leaves': 35.97155978086444}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_bo.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af837e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
