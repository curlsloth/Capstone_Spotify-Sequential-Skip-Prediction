{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e66449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import io\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open('../data/raw/20181120_track_features.tar.gz', 'r:gz')\n",
    "csv_files = tar.getnames()\n",
    "\n",
    "tf_df_list = []\n",
    "\n",
    "for csv_file in [csv_files[2], csv_files[4]]:\n",
    "    csv_contents = tar.extractfile(csv_file).read()\n",
    "    tf_df_list.append(pd.read_csv(io.BytesIO(csv_contents), encoding='utf8'))\n",
    "\n",
    "tf_df = pd.concat(tf_df_list, ignore_index=True)\n",
    "tf_df.rename(columns={'track_id':'track_id_clean'}, inplace=True)\n",
    "\n",
    "kmean300_df = pd.read_csv('../data/interim/all_data/mbKMeans300clusters.csv', usecols=['track_id','clus'])\n",
    "kmean300_df.rename(columns={'track_id':'track_id_clean'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510bbb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>datetime</th>\n",
       "      <th>bagging_fraction</th>\n",
       "      <th>bagging_freq</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>nFile</th>\n",
       "      <th>num_iterations</th>\n",
       "      <th>num_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.737572</td>\n",
       "      <td>{'datetime': '2022-12-14 09:47:13', 'elapsed':...</td>\n",
       "      <td>0.833968</td>\n",
       "      <td>1.357303</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.087423</td>\n",
       "      <td>40.0</td>\n",
       "      <td>664.099012</td>\n",
       "      <td>47.169789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.737376</td>\n",
       "      <td>{'datetime': '2022-12-14 09:09:26', 'elapsed':...</td>\n",
       "      <td>0.766158</td>\n",
       "      <td>3.669568</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.092895</td>\n",
       "      <td>40.0</td>\n",
       "      <td>653.033071</td>\n",
       "      <td>47.138080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.735923</td>\n",
       "      <td>{'datetime': '2022-12-14 05:08:57', 'elapsed':...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.892784</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>647.701054</td>\n",
       "      <td>47.199136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.735431</td>\n",
       "      <td>{'datetime': '2022-12-13 02:41:31', 'elapsed':...</td>\n",
       "      <td>0.755812</td>\n",
       "      <td>8.255519</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.070067</td>\n",
       "      <td>10.0</td>\n",
       "      <td>972.180853</td>\n",
       "      <td>18.477186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.735058</td>\n",
       "      <td>{'datetime': '2022-12-12 21:24:28', 'elapsed':...</td>\n",
       "      <td>0.787855</td>\n",
       "      <td>1.994299</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.094274</td>\n",
       "      <td>10.0</td>\n",
       "      <td>479.436874</td>\n",
       "      <td>23.859578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.734861</td>\n",
       "      <td>{'datetime': '2022-12-14 04:53:31', 'elapsed':...</td>\n",
       "      <td>0.801450</td>\n",
       "      <td>1.083993</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.079315</td>\n",
       "      <td>30.0</td>\n",
       "      <td>652.272230</td>\n",
       "      <td>39.776200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.733835</td>\n",
       "      <td>{'datetime': '2022-12-14 12:59:27', 'elapsed':...</td>\n",
       "      <td>0.608767</td>\n",
       "      <td>1.805601</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.082465</td>\n",
       "      <td>50.0</td>\n",
       "      <td>660.058584</td>\n",
       "      <td>45.575792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.733727</td>\n",
       "      <td>{'datetime': '2022-12-14 13:37:39', 'elapsed':...</td>\n",
       "      <td>0.872912</td>\n",
       "      <td>2.097131</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.054549</td>\n",
       "      <td>50.0</td>\n",
       "      <td>673.196402</td>\n",
       "      <td>49.448597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.732564</td>\n",
       "      <td>{'datetime': '2022-12-15 00:02:39', 'elapsed':...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>653.771650</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.732370</td>\n",
       "      <td>{'datetime': '2022-12-13 19:30:41', 'elapsed':...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>484.258199</td>\n",
       "      <td>25.485740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.732307</td>\n",
       "      <td>{'datetime': '2022-12-14 20:10:12', 'elapsed':...</td>\n",
       "      <td>0.624479</td>\n",
       "      <td>2.066166</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.086318</td>\n",
       "      <td>60.0</td>\n",
       "      <td>772.700232</td>\n",
       "      <td>49.937918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.732044</td>\n",
       "      <td>{'datetime': '2022-12-13 17:50:44', 'elapsed':...</td>\n",
       "      <td>0.683347</td>\n",
       "      <td>6.055283</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.090905</td>\n",
       "      <td>20.0</td>\n",
       "      <td>476.598104</td>\n",
       "      <td>26.100168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.721762</td>\n",
       "      <td>{'datetime': '2022-12-15 08:42:38', 'elapsed':...</td>\n",
       "      <td>0.876736</td>\n",
       "      <td>7.926591</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.072804</td>\n",
       "      <td>70.0</td>\n",
       "      <td>787.922998</td>\n",
       "      <td>49.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.721396</td>\n",
       "      <td>{'datetime': '2022-12-15 03:34:15', 'elapsed':...</td>\n",
       "      <td>0.721986</td>\n",
       "      <td>6.789434</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.072249</td>\n",
       "      <td>70.0</td>\n",
       "      <td>779.540382</td>\n",
       "      <td>49.522201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                           datetime  \\\n",
       "24  0.737572  {'datetime': '2022-12-14 09:47:13', 'elapsed':...   \n",
       "21  0.737376  {'datetime': '2022-12-14 09:09:26', 'elapsed':...   \n",
       "24  0.735923  {'datetime': '2022-12-14 05:08:57', 'elapsed':...   \n",
       "24  0.735431  {'datetime': '2022-12-13 02:41:31', 'elapsed':...   \n",
       "3   0.735058  {'datetime': '2022-12-12 21:24:28', 'elapsed':...   \n",
       "23  0.734861  {'datetime': '2022-12-14 04:53:31', 'elapsed':...   \n",
       "10  0.733835  {'datetime': '2022-12-14 12:59:27', 'elapsed':...   \n",
       "12  0.733727  {'datetime': '2022-12-14 13:37:39', 'elapsed':...   \n",
       "24  0.732564  {'datetime': '2022-12-15 00:02:39', 'elapsed':...   \n",
       "19  0.732370  {'datetime': '2022-12-13 19:30:41', 'elapsed':...   \n",
       "12  0.732307  {'datetime': '2022-12-14 20:10:12', 'elapsed':...   \n",
       "12  0.732044  {'datetime': '2022-12-13 17:50:44', 'elapsed':...   \n",
       "23  0.721762  {'datetime': '2022-12-15 08:42:38', 'elapsed':...   \n",
       "10  0.721396  {'datetime': '2022-12-15 03:34:15', 'elapsed':...   \n",
       "\n",
       "    bagging_fraction  bagging_freq  batch_size  learning_rate  nFile  \\\n",
       "24          0.833968      1.357303        10.0       0.087423   40.0   \n",
       "21          0.766158      3.669568        10.0       0.092895   40.0   \n",
       "24          0.900000      1.892784        10.0       0.100000   30.0   \n",
       "24          0.755812      8.255519        10.0       0.070067   10.0   \n",
       "3           0.787855      1.994299        10.0       0.094274   10.0   \n",
       "23          0.801450      1.083993        10.0       0.079315   30.0   \n",
       "10          0.608767      1.805601        10.0       0.082465   50.0   \n",
       "12          0.872912      2.097131        10.0       0.054549   50.0   \n",
       "24          0.900000      1.000000        10.0       0.100000   60.0   \n",
       "19          0.900000      1.000000        10.0       0.100000   20.0   \n",
       "12          0.624479      2.066166        10.0       0.086318   60.0   \n",
       "12          0.683347      6.055283        10.0       0.090905   20.0   \n",
       "23          0.876736      7.926591        10.0       0.072804   70.0   \n",
       "10          0.721986      6.789434        10.0       0.072249   70.0   \n",
       "\n",
       "    num_iterations  num_leaves  \n",
       "24      664.099012   47.169789  \n",
       "21      653.033071   47.138080  \n",
       "24      647.701054   47.199136  \n",
       "24      972.180853   18.477186  \n",
       "3       479.436874   23.859578  \n",
       "23      652.272230   39.776200  \n",
       "10      660.058584   45.575792  \n",
       "12      673.196402   49.448597  \n",
       "24      653.771650   50.000000  \n",
       "19      484.258199   25.485740  \n",
       "12      772.700232   49.937918  \n",
       "12      476.598104   26.100168  \n",
       "23      787.922998   49.000702  \n",
       "10      779.540382   49.522201  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "list_bayesOpt = glob.glob('../models/SVD/LightGBM_BayesOpt_dec12/logs_shuffle*.json')\n",
    "\n",
    "opt_best_df = pd.DataFrame()\n",
    "for jsonFile in list_bayesOpt:\n",
    "    with open(jsonFile) as f:\n",
    "        optList = []\n",
    "        for jsonObj in f:\n",
    "            optDict = json.loads(jsonObj)\n",
    "            optList.append(optDict)\n",
    "        \n",
    "        opt_df = pd.DataFrame(optList)\n",
    "        opt_df = pd.concat([opt_df.drop(['params'], axis=1), opt_df['params'].apply(pd.Series)], axis=1)\n",
    "        opt_best_df = pd.concat([opt_best_df,opt_df.sort_values('target',ascending=False).iloc[0:2]])\n",
    "\n",
    "opt_best_df.sort_values('target',ascending=False)\n",
    "# opt_df = pd.DataFrame(optList)\n",
    "# opt_df = pd.concat([opt_df.drop(['params'], axis=1), opt_df['params'].apply(pd.Series)], axis=1)\n",
    "# opt_df.sort_values('target',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851b1686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target                0.732588\n",
       "bagging_fraction      0.788105\n",
       "bagging_freq          3.356691\n",
       "batch_size           10.000000\n",
       "learning_rate         0.084519\n",
       "nFile                40.000000\n",
       "num_iterations      668.340689\n",
       "num_leaves           40.620792\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_best_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86fcb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target                0.733781\n",
       "bagging_fraction      0.794653\n",
       "bagging_freq          2.030233\n",
       "batch_size           10.000000\n",
       "learning_rate         0.086871\n",
       "nFile                40.000000\n",
       "num_iterations      656.915117\n",
       "num_leaves           47.153935\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_best_df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff380d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(df_hist, df_lookup, sim_file_list, score_name_list):\n",
    "    df_hist['ListenYes'] = (df_hist['skip_2'] == False)*1\n",
    "    df_hist['ListenYes'].replace(0, -1, inplace = True)\n",
    "    df_hist = df_hist.groupby(['session_id', 'clus']).agg({'ListenYes':['sum']})\n",
    "    df_hist = df_hist.reset_index()\n",
    "    df_hist.columns = df_hist.columns.droplevel(level = 1) # take out the unwanted level\n",
    "    df_pivot = pd.pivot_table(df_hist, values = 'ListenYes',index='session_id', columns='clus')\n",
    "    df_pivot = df_pivot.fillna(0)\n",
    "    \n",
    "    \n",
    "    for sim_file, score_name in zip(sim_file_list, score_name_list):\n",
    "        sim_matrix = pd.read_csv(sim_file).drop(columns=['Unnamed: 0'])\n",
    "        sim_matrix.columns = list(map(str, range(0,len(sim_matrix))))\n",
    "        df_sim_session = df_pivot.dot(sim_matrix)/sim_matrix.sum()\n",
    "        \n",
    "        df_lookup[score_name] = df_sim_session.lookup(df_lookup['session_id'],df_lookup['clus'].astype(str))\n",
    "    \n",
    "    return df_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc506af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "for n in range(9):\n",
    "    file_list = file_list + glob.glob('../data/raw/training_set/log_'+str(n)+'*.csv')\n",
    "\n",
    "import random\n",
    "random.Random(23).shuffle(file_list) # randomly shuffle the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd8b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dfs(file, tf_df):\n",
    "    log_df = pd.read_csv(file)\n",
    "    log_df = log_df.merge(kmean300_df)\n",
    "\n",
    "    log_df_1 = log_df.loc[log_df['session_position']<=(log_df['session_length']/2)]\n",
    "    log_df_1['hour_of_day'] = log_df_1['hour_of_day'].astype('float')\n",
    "    log_df_1['premium'] = log_df_1['premium'].astype('bool')\n",
    "    log_df_1['weekday'] = log_df_1['date'].astype('datetime64[ns]').dt.dayofweek\n",
    "    log_df_1 = log_df_1.drop(columns = ['date'])\n",
    "    log_df_1 = pd.get_dummies(log_df_1, columns=['hist_user_behavior_reason_end', 'hist_user_behavior_reason_start', 'context_type','weekday'], dtype = 'bool')\n",
    "    log_df_1 = log_df_1.merge(tf_df.drop(columns = ['time_signature','mode','key']))\n",
    "    \n",
    "                     \n",
    "    col_bool = log_df_1.columns[log_df_1.dtypes=='bool']\n",
    "    col_nonbool = log_df_1.columns[log_df_1.dtypes!='bool'].drop(['session_id','track_id_clean','clus'])\n",
    "    \n",
    "    # the non-convertable values will be set to 0\n",
    "    log_df_1[col_nonbool] = log_df_1[col_nonbool].apply(pd.to_numeric, errors='coerce', downcast = 'float').fillna(0).astype('float32')\n",
    "\n",
    "    # aggregate the track history where ['skip_2']==True\n",
    "    log_df_1_summary_skip2True = pd.concat([log_df_1.loc[log_df_1['skip_2']==True].groupby(['session_id'])[col_bool].agg(['mean']), \n",
    "                                            log_df_1.loc[log_df_1['skip_2']==True].groupby(['session_id'])[col_nonbool].agg(['mean', 'std', 'median'])],\n",
    "                                            axis = 1)\n",
    "    log_df_1_summary_skip2True.columns = log_df_1_summary_skip2True.columns.get_level_values(0)+'_sk2True_'+log_df_1_summary_skip2True.columns.get_level_values(1)\n",
    "    \n",
    "    # aggregate the track history where ['skip_2']==False\n",
    "    log_df_1_summary_skip2False = pd.concat([log_df_1.loc[log_df_1['skip_2']==False].groupby(['session_id'])[col_bool].agg(['mean']), \n",
    "                                             log_df_1.loc[log_df_1['skip_2']==False].groupby(['session_id'])[col_nonbool].agg(['mean', 'std', 'median'])],\n",
    "                                             axis = 1)\n",
    "    log_df_1_summary_skip2False.columns = log_df_1_summary_skip2False.columns.get_level_values(0)+'_sk2False_'+log_df_1_summary_skip2False.columns.get_level_values(1)\n",
    "    \n",
    "    \n",
    "    log_df_history = log_df_1[['session_id','track_id_clean','skip_2','clus']]\n",
    "\n",
    "\n",
    "    half_cut = log_df['session_length']/2\n",
    "\n",
    "    # need to at least include 2 trials, otherwise the log_df_1_summary will confound with all the tracks in the same session\n",
    "\n",
    "    #1st trial in the 2nd half\n",
    "    log_df_2_1 = log_df.loc[(log_df['session_position']>half_cut) & (log_df['session_position']<=half_cut+1)]\n",
    "    log_df_2_1 = log_df_2_1[['session_id','track_id_clean','skip_2','session_position','session_length','clus']]\n",
    "    log_df_2_1['weight'] = 1\n",
    "\n",
    "    #2nd trial in the 2nd half\n",
    "    log_df_2_2 = log_df.loc[(log_df['session_position']>half_cut+1) & (log_df['session_position']<=half_cut+2)]\n",
    "    log_df_2_2 = log_df_2_2[['session_id','track_id_clean','skip_2','session_position','session_length','clus']]\n",
    "    log_df_2_2['weight'] = 0.75\n",
    "\n",
    "    log_df_2 = pd.concat([log_df_2_1,log_df_2_2])\n",
    "    log_df_2 = log_df_2.merge(log_df_1_summary_skip2True, on='session_id')\n",
    "    log_df_2 = log_df_2.merge(log_df_1_summary_skip2False, on='session_id')\n",
    "\n",
    "    sim_file_list = ['../models/SVD/all_tracks/similarity/k300_CanbDist.csv',\n",
    "                     '../models/SVD/all_tracks/similarity/k300_CosSim.csv',\n",
    "                     '../models/SVD/all_tracks/similarity/k300_LinCorr.csv',\n",
    "                     '../models/SVD/all_tracks/similarity/k300_ManhDist.csv',\n",
    "                     '../models/SVD/all_tracks/similarity/k300_HammDist.csv',\n",
    "                     '../models/SVD/all_tracks/similarity/k300_SpearCorr.csv',\n",
    "                     '../models/SVD/all_tracks/similarity/k300_KendCorr.csv',\n",
    "                     '../models/SVD/all_tracks/similarity/k300_ChebDist.csv',\n",
    "                     '../models/SVD/all_tracks/similarity/k300_BrayDist.csv']\n",
    "    score_name_list = ['CanbDist300', 'CosSim300','LinCorr300','ManhDist300','HammDist300','SpearCorr300','KendCorr300','ChebDist','BrayDist']\n",
    "\n",
    "    return get_sim(log_df_history, log_df_2, sim_file_list, score_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbde35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime per batch: 739.76s\n",
      "Runtime per batch: 631.04s\n",
      "Runtime per batch: 605.05s\n",
      "Runtime per batch: 716.47s\n",
      "Runtime per batch: 696.40s\n",
      "Runtime per batch: 682.61s\n",
      "Runtime per batch: 756.11s\n",
      "Runtime per batch: 744.21s\n",
      "Runtime per batch: 730.16s\n",
      "Runtime per batch: 727.42s\n",
      "Runtime per batch: 787.79s\n",
      "Runtime per batch: 776.63s\n",
      "Runtime per batch: 883.42s\n",
      "Runtime per batch: 897.19s\n",
      "Runtime per batch: 993.23s\n",
      "Runtime per batch: 1133.09s\n",
      "Runtime per batch: 1028.33s\n",
      "Runtime per batch: 1174.07s\n",
      "Runtime per batch: 1281.22s\n",
      "Runtime per batch: 1363.78s\n",
      "Runtime per batch: 1415.34s\n",
      "Runtime per batch: 1455.43s\n",
      "Runtime per batch: 1697.62s\n",
      "Runtime per batch: 1612.20s\n",
      "Runtime per batch: 1679.12s\n",
      "Runtime per batch: 1768.61s\n",
      "Runtime per batch: 1833.88s\n",
      "Runtime per batch: 1861.30s\n",
      "Runtime per batch: 2001.38s\n",
      "Runtime per batch: 2133.81s\n",
      "Runtime per batch: 2385.80s\n",
      "Runtime per batch: 2178.07s\n",
      "Runtime per batch: 2294.76s\n",
      "Runtime per batch: 2272.99s\n",
      "Runtime per batch: 2340.99s\n",
      "Runtime per batch: 2467.31s\n",
      "Runtime per batch: 2434.32s\n",
      "Runtime per batch: 2376.30s\n",
      "Runtime per batch: 2650.41s\n",
      "Runtime per batch: 2555.09s\n",
      "Runtime per batch: 2965.41s\n",
      "Runtime per batch: 2765.64s\n",
      "Runtime per batch: 2940.01s\n",
      "Runtime per batch: 2668.60s\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from timeit import default_timer as timer #to see how long the computation will take\n",
    "\n",
    "nFile = 0\n",
    "batch_size = 10\n",
    "while nFile < len(file_list):\n",
    "    start = timer()\n",
    "    nFile += batch_size\n",
    "    df_lookup_list = []\n",
    "    for file in file_list[(nFile-batch_size):min(nFile, len(file_list))]:\n",
    "        df_lookup_list.append(prep_dfs(file, tf_df))\n",
    "\n",
    "    df_lookup = pd.concat(df_lookup_list)\n",
    "    df_lookup = df_lookup.merge(tf_df.drop(columns = ['key','time_signature','mode']))\n",
    "    \n",
    "    # check whether the column names match with the previous training set\n",
    "    if nFile>batch_size:\n",
    "        prev_feature_names = lgb.Booster(model_file='../models/SVD/LightGBM_BayesOpt_dec12/LightGBM_incremental_training_dec16/boost'+str(int(nFile-batch_size))+'.txt').feature_name()\n",
    "        if bool(set(prev_feature_names) - set(df_lookup.columns)): # if there are missing columns\n",
    "            df_lookup[list(set(prev_feature_names) - set(df_lookup.columns))] = 0 # add the missed columns with 0\n",
    "\n",
    "    dtrain = lgb.Dataset(df_lookup.drop(columns = ['session_id','track_id_clean','skip_2','weight']).astype('float32'), \n",
    "                     label=df_lookup['skip_2'],\n",
    "                     weight = df_lookup['weight'],\n",
    "                     free_raw_data=False) # https://lightgbm.readthedocs.io/en/latest/FAQ.html#error-messages-cannot-before-construct-dataset\n",
    "\n",
    "    \n",
    "    params = {'num_leaves': 24,\n",
    "              'learning_rate':0.09,\n",
    "              'metric': 'binary_error',\n",
    "              'num_iterations':480,\n",
    "              'bagging_fraction':0.8,\n",
    "              'bagging_freq':2,\n",
    "              'objective': 'binary',\n",
    "              'force_row_wise': True,\n",
    "              'num_threads': 5,\n",
    "              'verbosity': 0,\n",
    "              'tree_learner': 'voting'} #https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html\n",
    "    \n",
    "    if nFile == batch_size:\n",
    "        bst = lgb.train(params, dtrain)\n",
    "    else: # continue training on the previous model\n",
    "        bst = lgb.train(params, dtrain, init_model='../models/SVD/LightGBM_BayesOpt_dec12/LightGBM_incremental_training_dec16/boost'+str(int(nFile-batch_size))+'.txt')\n",
    "        \n",
    "    bst.save_model('../models/SVD/LightGBM_BayesOpt_dec12/LightGBM_incremental_training_dec16/boost'+str(int(nFile))+'.txt')\n",
    "\n",
    "    print('Runtime per batch: %0.2fs' % (timer() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2792e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af837e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
