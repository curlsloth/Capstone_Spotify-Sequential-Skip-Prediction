{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e66449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import io\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import myFunc as mf\n",
    "\n",
    "tar = tarfile.open('../data/raw/20181120_track_features.tar.gz', 'r:gz')\n",
    "csv_files = tar.getnames()\n",
    "\n",
    "tf_df_list = []\n",
    "\n",
    "for csv_file in [csv_files[2], csv_files[4]]:\n",
    "    csv_contents = tar.extractfile(csv_file).read()\n",
    "    tf_df_list.append(pd.read_csv(io.BytesIO(csv_contents), encoding='utf8'))\n",
    "\n",
    "tf_df = pd.concat(tf_df_list, ignore_index=True)\n",
    "tf_df.rename(columns={'track_id':'track_id_clean'}, inplace=True)\n",
    "\n",
    "kmean300_df = pd.read_csv('../data/interim/all_data/mbKMeans300clusters.csv', usecols=['track_id','clus'])\n",
    "kmean300_df.rename(columns={'track_id':'track_id_clean'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510bbb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>datetime</th>\n",
       "      <th>bagging_fraction</th>\n",
       "      <th>bagging_freq</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>feature_fraction</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>min_gain_to_split</th>\n",
       "      <th>nFile</th>\n",
       "      <th>num_iterations</th>\n",
       "      <th>num_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.720543</td>\n",
       "      <td>{'datetime': '2022-12-20 20:47:08', 'elapsed':...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.731797</td>\n",
       "      <td>0.153480</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>850.978738</td>\n",
       "      <td>25.156269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.720360</td>\n",
       "      <td>{'datetime': '2022-12-21 03:42:12', 'elapsed':...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.435503</td>\n",
       "      <td>0.226245</td>\n",
       "      <td>0.088026</td>\n",
       "      <td>10.0</td>\n",
       "      <td>859.312532</td>\n",
       "      <td>29.996526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>{'datetime': '2022-12-20 22:40:18', 'elapsed':...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.798139</td>\n",
       "      <td>0.153018</td>\n",
       "      <td>0.028336</td>\n",
       "      <td>10.0</td>\n",
       "      <td>842.720473</td>\n",
       "      <td>25.606681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.720302</td>\n",
       "      <td>{'datetime': '2022-12-20 18:55:13', 'elapsed':...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.673718</td>\n",
       "      <td>0.346856</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>10.0</td>\n",
       "      <td>842.212623</td>\n",
       "      <td>27.897216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.720035</td>\n",
       "      <td>{'datetime': '2022-12-21 00:07:55', 'elapsed':...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.728087</td>\n",
       "      <td>0.355348</td>\n",
       "      <td>0.078096</td>\n",
       "      <td>10.0</td>\n",
       "      <td>858.867160</td>\n",
       "      <td>27.280231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                           datetime  \\\n",
       "20  0.720543  {'datetime': '2022-12-20 20:47:08', 'elapsed':...   \n",
       "33  0.720360  {'datetime': '2022-12-21 03:42:12', 'elapsed':...   \n",
       "24  0.720333  {'datetime': '2022-12-20 22:40:18', 'elapsed':...   \n",
       "15  0.720302  {'datetime': '2022-12-20 18:55:13', 'elapsed':...   \n",
       "27  0.720035  {'datetime': '2022-12-21 00:07:55', 'elapsed':...   \n",
       "\n",
       "    bagging_fraction  bagging_freq  batch_size  feature_fraction  \\\n",
       "20               0.8           2.0        10.0          0.731797   \n",
       "33               0.8           2.0        10.0          0.435503   \n",
       "24               0.8           2.0        10.0          0.798139   \n",
       "15               0.8           2.0        10.0          0.673718   \n",
       "27               0.8           2.0        10.0          0.728087   \n",
       "\n",
       "    learning_rate  min_gain_to_split  nFile  num_iterations  num_leaves  \n",
       "20       0.153480           0.089333   10.0      850.978738   25.156269  \n",
       "33       0.226245           0.088026   10.0      859.312532   29.996526  \n",
       "24       0.153018           0.028336   10.0      842.720473   25.606681  \n",
       "15       0.346856           0.013443   10.0      842.212623   27.897216  \n",
       "27       0.355348           0.078096   10.0      858.867160   27.280231  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the 5 parameter sets with the best performance\n",
    "\n",
    "import json\n",
    "\n",
    "list_bayesOpt = glob.glob('../models/SVD/LightGBM_BayesOpt_dec17/for20180918/logs_shuffle10.json')\n",
    "\n",
    "opt_best_df = pd.DataFrame()\n",
    "for jsonFile in list_bayesOpt:\n",
    "    with open(jsonFile) as f:\n",
    "        optList = []\n",
    "        for jsonObj in f:\n",
    "            optDict = json.loads(jsonObj)\n",
    "            optList.append(optDict)\n",
    "        \n",
    "        opt_df = pd.DataFrame(optList)\n",
    "        opt_df = pd.concat([opt_df.drop(['params'], axis=1), opt_df['params'].apply(pd.Series)], axis=1)\n",
    "        opt_best_df = pd.concat([opt_best_df,opt_df.sort_values('target',ascending=False).iloc[0:5]])\n",
    "\n",
    "opt_best_df.sort_values('target',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa251606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/raw/training_set/log_7_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_8_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_5_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_6_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_3_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_2_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_9_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_0_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_1_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_4_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_7_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_8_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_5_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_6_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_3_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_2_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_9_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_0_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_1_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_4_20180916_000000000000.csv',\n",
       " '../data/raw/training_set/log_7_20180915_000000000000.csv',\n",
       " '../data/raw/training_set/log_8_20180915_000000000000.csv',\n",
       " '../data/raw/training_set/log_5_20180915_000000000000.csv',\n",
       " '../data/raw/training_set/log_6_20180915_000000000000.csv',\n",
       " '../data/raw/training_set/log_3_20180915_000000000000.csv',\n",
       " '../data/raw/training_set/log_2_20180915_000000000000.csv',\n",
       " '../data/raw/training_set/log_9_20180915_000000000000.csv',\n",
       " '../data/raw/training_set/log_0_20180915_000000000000.csv',\n",
       " '../data/raw/training_set/log_1_20180915_000000000000.csv',\n",
       " '../data/raw/training_set/log_4_20180915_000000000000.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "file_list = []\n",
    "\n",
    "temp_list = []\n",
    "for logN in range(10):\n",
    "    temp_list.append('../data/raw/training_set/log_'+str(logN)+'_20180917_000000000000.csv')\n",
    "\n",
    "random.Random(23).shuffle(temp_list)\n",
    "file_list += temp_list\n",
    "\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "for logN in range(10):\n",
    "    temp_list.append('../data/raw/training_set/log_'+str(logN)+'_20180916_000000000000.csv')\n",
    "\n",
    "random.Random(23).shuffle(temp_list)\n",
    "file_list += temp_list\n",
    "\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "for logN in range(10):\n",
    "    temp_list.append('../data/raw/training_set/log_'+str(logN)+'_20180915_000000000000.csv')\n",
    "\n",
    "random.Random(23).shuffle(temp_list)\n",
    "file_list += temp_list\n",
    "\n",
    "\n",
    "\n",
    "# temp_list = []\n",
    "# for logN in range(10):\n",
    "#     temp_list.append('../data/raw/training_set/log_'+str(logN)+'_20180914_000000000000.csv')\n",
    "\n",
    "# random.Random(23).shuffle(temp_list)\n",
    "# file_list += temp_list\n",
    "\n",
    "\n",
    "\n",
    "# temp_list = []\n",
    "# for logN in range(10):\n",
    "#     temp_list.append('../data/raw/training_set/log_'+str(logN)+'_20180913_000000000000.csv')\n",
    "\n",
    "# random.Random(23).shuffle(temp_list)\n",
    "# file_list += temp_list\n",
    "\n",
    "\n",
    "\n",
    "# temp_list = []\n",
    "# for logN in range(10):\n",
    "#     temp_list.append('../data/raw/training_set/log_'+str(logN)+'_20180912_000000000000.csv')\n",
    "\n",
    "# random.Random(23).shuffle(temp_list)\n",
    "# file_list += temp_list\n",
    "\n",
    "\n",
    "\n",
    "# temp_list = []\n",
    "# for logN in range(10):\n",
    "#     temp_list.append('../data/raw/training_set/log_'+str(logN)+'_20180911_000000000000.csv')\n",
    "\n",
    "# random.Random(23).shuffle(temp_list)\n",
    "# file_list += temp_list\n",
    "    \n",
    "\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbde35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from timeit import default_timer as timer #to see how long the computation will take\n",
    "\n",
    "\n",
    "nFile = 0\n",
    "batch_size = 5\n",
    "while nFile < len(file_list):\n",
    "    start = timer()\n",
    "    nFile += batch_size\n",
    "    df_lookup_list = []\n",
    "    for file in file_list[(nFile-batch_size):min(nFile, len(file_list))]:\n",
    "        \n",
    "        if nFile<=10:\n",
    "            simFolder = 'similarity_20180917-20180917'\n",
    "        elif nFile<=20:\n",
    "            simFolder = 'similarity_20180916-20180917'\n",
    "        elif nFile<=30:\n",
    "            simFolder = 'similarity_20180915-20180917'\n",
    "#         elif nFile<=40:\n",
    "#             simFolder = 'similarity_20180914-20180917'\n",
    "#         elif nFile<=50:\n",
    "#             simFolder = 'similarity_20180913-20180917'\n",
    "#         elif nFile<=60:\n",
    "#             simFolder = 'similarity_20180912-20180917'\n",
    "#         elif nFile<=70:\n",
    "#             simFolder = 'similarity_20180911-20180917'\n",
    "            \n",
    "            \n",
    "        df_lookup_list.append(mf.prep_dfs(file, tf_df, simFolder, kmean300_df, track6 = True))\n",
    "\n",
    "    df_lookup = pd.concat(df_lookup_list)\n",
    "    df_lookup = df_lookup.merge(tf_df.drop(columns = ['key','time_signature','mode']))\n",
    "    \n",
    "    # check whether the column names match with the previous training set\n",
    "    if nFile>batch_size:\n",
    "        prev_feature_names = lgb.Booster(model_file='../models/SVD/LightGBM_BayesOpt_dec17/for20180918/boost_alltracks_incrementalTrain_5_multidays_jan14.txt').feature_name()\n",
    "        if bool(set(prev_feature_names) - set(df_lookup.columns)): # if there are missing columns\n",
    "            df_lookup[list(set(prev_feature_names) - set(df_lookup.columns))] = 0 # add the missed columns with 0\n",
    "            \n",
    "        if bool(set(df_lookup.columns)- set(prev_feature_names)): # if there are extra columns\n",
    "            extra_cols = list(set(df_lookup.columns)- set(prev_feature_names) - set(['session_id','track_id_clean','skip_2','weight']))\n",
    "            df_lookup.drop(columns = extra_cols, inplace = True)\n",
    "    \n",
    "\n",
    "    dtrain = lgb.Dataset(df_lookup.drop(columns = ['session_id','track_id_clean','skip_2','weight']).astype('float32'), \n",
    "                     label=df_lookup['skip_2'],\n",
    "                     weight = df_lookup['weight'],\n",
    "                     free_raw_data=False) # https://lightgbm.readthedocs.io/en/latest/FAQ.html#error-messages-cannot-before-construct-dataset\n",
    "\n",
    "    \n",
    "    params = {'num_leaves': 25,\n",
    "              'learning_rate':0.15,\n",
    "              'metric': 'binary_error',\n",
    "              'num_iterations':851,\n",
    "              'bagging_fraction':0.8,\n",
    "              'bagging_freq':2,\n",
    "              'feature_fraction':0.73,\n",
    "              'min_gain_to_split':0.09,\n",
    "              'objective': 'binary',\n",
    "              'force_row_wise': True,\n",
    "              'num_threads': 5,\n",
    "              'verbosity': 0,\n",
    "              'tree_learner': 'voting_parallel'} #https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html\n",
    "    \n",
    "    if nFile == batch_size:\n",
    "        bst = lgb.train(params, dtrain)\n",
    "    else: # continue training on the previous model\n",
    "        bst = lgb.train(params, dtrain, init_model='../models/SVD/LightGBM_BayesOpt_dec17/for20180918/boost_alltracks_incrementalTrain_'+str(int(nFile-batch_size))+'_multidays_jan14.txt')\n",
    "        \n",
    "    bst.save_model('../models/SVD/LightGBM_BayesOpt_dec17/for20180918/boost_alltracks_incrementalTrain_'+str(int(nFile))+'_multidays_jan14.txt')\n",
    "\n",
    "    print('Runtime per batch: %0.2fs' % (timer() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d17bc6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
