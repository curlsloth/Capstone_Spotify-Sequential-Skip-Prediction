{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e66449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import io\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "tar = tarfile.open('../data/raw/20181120_track_features.tar.gz', 'r:gz')\n",
    "csv_files = tar.getnames()\n",
    "\n",
    "tf_df_list = []\n",
    "\n",
    "for csv_file in [csv_files[2], csv_files[4]]:\n",
    "    csv_contents = tar.extractfile(csv_file).read()\n",
    "    tf_df_list.append(pd.read_csv(io.BytesIO(csv_contents), encoding='utf8'))\n",
    "\n",
    "tf_df = pd.concat(tf_df_list, ignore_index=True)\n",
    "tf_df.rename(columns={'track_id':'track_id_clean'}, inplace=True)\n",
    "\n",
    "kmean300_df = pd.read_csv('../data/interim/all_data/mbKMeans300clusters.csv', usecols=['track_id','clus'])\n",
    "kmean300_df.rename(columns={'track_id':'track_id_clean'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510bbb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>datetime</th>\n",
       "      <th>bagging_fraction</th>\n",
       "      <th>bagging_freq</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>feature_fraction</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>min_gain_to_split</th>\n",
       "      <th>nFile</th>\n",
       "      <th>num_iterations</th>\n",
       "      <th>num_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.709340</td>\n",
       "      <td>{'datetime': '2023-01-01 03:14:04', 'elapsed':...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1285.347539</td>\n",
       "      <td>29.979064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.708748</td>\n",
       "      <td>{'datetime': '2022-12-31 15:27:01', 'elapsed':...</td>\n",
       "      <td>0.996391</td>\n",
       "      <td>2.845256</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.752420</td>\n",
       "      <td>0.176366</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1287.127592</td>\n",
       "      <td>28.751427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.708179</td>\n",
       "      <td>{'datetime': '2022-12-31 22:36:38', 'elapsed':...</td>\n",
       "      <td>0.845199</td>\n",
       "      <td>1.135083</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.759154</td>\n",
       "      <td>0.170960</td>\n",
       "      <td>0.341642</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1282.532117</td>\n",
       "      <td>28.081893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.708109</td>\n",
       "      <td>{'datetime': '2022-12-31 17:27:44', 'elapsed':...</td>\n",
       "      <td>0.804886</td>\n",
       "      <td>1.825086</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.749743</td>\n",
       "      <td>0.173780</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1285.271190</td>\n",
       "      <td>27.817761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.707983</td>\n",
       "      <td>{'datetime': '2022-12-30 03:39:57', 'elapsed':...</td>\n",
       "      <td>0.810146</td>\n",
       "      <td>2.518164</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.696611</td>\n",
       "      <td>0.191192</td>\n",
       "      <td>0.360641</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1320.701485</td>\n",
       "      <td>25.176046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.707924</td>\n",
       "      <td>{'datetime': '2022-12-30 11:06:25', 'elapsed':...</td>\n",
       "      <td>0.854251</td>\n",
       "      <td>1.666522</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.761067</td>\n",
       "      <td>0.177636</td>\n",
       "      <td>0.068465</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1320.815612</td>\n",
       "      <td>24.265653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.707638</td>\n",
       "      <td>{'datetime': '2022-12-30 13:33:50', 'elapsed':...</td>\n",
       "      <td>0.851072</td>\n",
       "      <td>3.586118</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.789447</td>\n",
       "      <td>0.178652</td>\n",
       "      <td>0.357470</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1320.841196</td>\n",
       "      <td>23.962685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.707515</td>\n",
       "      <td>{'datetime': '2022-12-31 21:39:52', 'elapsed':...</td>\n",
       "      <td>0.805704</td>\n",
       "      <td>2.694404</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.603076</td>\n",
       "      <td>0.141578</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1288.624879</td>\n",
       "      <td>29.817010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.707468</td>\n",
       "      <td>{'datetime': '2022-12-31 07:29:13', 'elapsed':...</td>\n",
       "      <td>0.977039</td>\n",
       "      <td>3.144350</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.686310</td>\n",
       "      <td>0.154774</td>\n",
       "      <td>0.317477</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1292.239605</td>\n",
       "      <td>26.283170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.707464</td>\n",
       "      <td>{'datetime': '2022-12-30 16:47:17', 'elapsed':...</td>\n",
       "      <td>0.780234</td>\n",
       "      <td>1.657787</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.611614</td>\n",
       "      <td>0.194624</td>\n",
       "      <td>0.350407</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1322.425403</td>\n",
       "      <td>24.333708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                           datetime  \\\n",
       "49  0.709340  {'datetime': '2023-01-01 03:14:04', 'elapsed':...   \n",
       "38  0.708748  {'datetime': '2022-12-31 15:27:01', 'elapsed':...   \n",
       "45  0.708179  {'datetime': '2022-12-31 22:36:38', 'elapsed':...   \n",
       "40  0.708109  {'datetime': '2022-12-31 17:27:44', 'elapsed':...   \n",
       "5   0.707983  {'datetime': '2022-12-30 03:39:57', 'elapsed':...   \n",
       "12  0.707924  {'datetime': '2022-12-30 11:06:25', 'elapsed':...   \n",
       "14  0.707638  {'datetime': '2022-12-30 13:33:50', 'elapsed':...   \n",
       "44  0.707515  {'datetime': '2022-12-31 21:39:52', 'elapsed':...   \n",
       "31  0.707468  {'datetime': '2022-12-31 07:29:13', 'elapsed':...   \n",
       "17  0.707464  {'datetime': '2022-12-30 16:47:17', 'elapsed':...   \n",
       "\n",
       "    bagging_fraction  bagging_freq  batch_size  feature_fraction  \\\n",
       "49          1.000000      1.000000        10.0          0.800000   \n",
       "38          0.996391      2.845256        10.0          0.752420   \n",
       "45          0.845199      1.135083        10.0          0.759154   \n",
       "40          0.804886      1.825086        10.0          0.749743   \n",
       "5           0.810146      2.518164        10.0          0.696611   \n",
       "12          0.854251      1.666522        10.0          0.761067   \n",
       "14          0.851072      3.586118        10.0          0.789447   \n",
       "44          0.805704      2.694404        10.0          0.603076   \n",
       "31          0.977039      3.144350        10.0          0.686310   \n",
       "17          0.780234      1.657787        10.0          0.611614   \n",
       "\n",
       "    learning_rate  min_gain_to_split  nFile  num_iterations  num_leaves  \n",
       "49       0.200000           0.400000   10.0     1285.347539   29.979064  \n",
       "38       0.176366           0.016599   10.0     1287.127592   28.751427  \n",
       "45       0.170960           0.341642   10.0     1282.532117   28.081893  \n",
       "40       0.173780           0.097917   10.0     1285.271190   27.817761  \n",
       "5        0.191192           0.360641   10.0     1320.701485   25.176046  \n",
       "12       0.177636           0.068465   10.0     1320.815612   24.265653  \n",
       "14       0.178652           0.357470   10.0     1320.841196   23.962685  \n",
       "44       0.141578           0.023728   10.0     1288.624879   29.817010  \n",
       "31       0.154774           0.317477   10.0     1292.239605   26.283170  \n",
       "17       0.194624           0.350407   10.0     1322.425403   24.333708  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "list_bayesOpt = glob.glob('../models/SVD/LightGBM_BayesOpt_dec17/for20180918/logs_alltracks_10_1dayAhead.json')\n",
    "\n",
    "opt_best_df = pd.DataFrame()\n",
    "for jsonFile in list_bayesOpt:\n",
    "    with open(jsonFile) as f:\n",
    "        optList = []\n",
    "        for jsonObj in f:\n",
    "            optDict = json.loads(jsonObj)\n",
    "            optList.append(optDict)\n",
    "        \n",
    "        opt_df = pd.DataFrame(optList)\n",
    "        opt_df = pd.concat([opt_df.drop(['params'], axis=1), opt_df['params'].apply(pd.Series)], axis=1)\n",
    "        opt_best_df = pd.concat([opt_best_df,opt_df.sort_values('target',ascending=False).iloc[0:10]])\n",
    "\n",
    "opt_best_df.sort_values('target',ascending=False)\n",
    "# opt_df = pd.DataFrame(optList)\n",
    "# opt_df = pd.concat([opt_df.drop(['params'], axis=1), opt_df['params'].apply(pd.Series)], axis=1)\n",
    "# opt_df.sort_values('target',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851b1686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target                  0.708037\n",
       "bagging_fraction        0.872492\n",
       "bagging_freq            2.207277\n",
       "batch_size             10.000000\n",
       "feature_fraction        0.720944\n",
       "learning_rate           0.175956\n",
       "min_gain_to_split       0.233435\n",
       "nFile                  10.000000\n",
       "num_iterations       1300.592662\n",
       "num_leaves             26.846842\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_best_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86fcb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target                  0.707953\n",
       "bagging_fraction        0.848135\n",
       "bagging_freq            2.171625\n",
       "batch_size             10.000000\n",
       "feature_fraction        0.751082\n",
       "learning_rate           0.177001\n",
       "min_gain_to_split       0.329559\n",
       "nFile                  10.000000\n",
       "num_iterations       1290.432242\n",
       "num_leaves             27.050466\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_best_df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff380d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(df_hist, df_lookup, sim_file_list, score_name_list):\n",
    "    df_hist['ListenYes'] = (df_hist['skip_2'] == False)*1\n",
    "    df_hist['ListenYes'].replace(0, -1, inplace = True)\n",
    "    df_hist = df_hist.groupby(['session_id', 'clus']).agg({'ListenYes':['sum']})\n",
    "    df_hist = df_hist.reset_index()\n",
    "    df_hist.columns = df_hist.columns.droplevel(level = 1) # take out the unwanted level\n",
    "    df_pivot = pd.pivot_table(df_hist, values = 'ListenYes',index='session_id', columns='clus')\n",
    "    df_pivot = df_pivot.fillna(0)\n",
    "    \n",
    "    \n",
    "    for sim_file, score_name in zip(sim_file_list, score_name_list):\n",
    "        sim_matrix = pd.read_csv(sim_file).drop(columns=['Unnamed: 0'])\n",
    "        sim_matrix.columns = list(map(str, range(0,len(sim_matrix))))\n",
    "        df_sim_session = df_pivot.dot(sim_matrix)/sim_matrix.sum()\n",
    "        \n",
    "        df_lookup[score_name] = df_sim_session.lookup(df_lookup['session_id'],df_lookup['clus'].astype(str))\n",
    "    \n",
    "    return df_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa251606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/raw/training_set/log_7_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_8_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_5_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_6_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_3_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_2_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_9_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_0_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_1_20180917_000000000000.csv',\n",
       " '../data/raw/training_set/log_4_20180917_000000000000.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "file_list = []\n",
    "\n",
    "temp_list = []\n",
    "for logN in range(10):\n",
    "    temp_list.append('../data/raw/training_set/log_'+str(logN)+'_20180917_000000000000.csv')\n",
    "\n",
    "random.Random(23).shuffle(temp_list)\n",
    "file_list += temp_list\n",
    "    \n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c65a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd8b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dfs(file, tf_df):\n",
    "    log_df = pd.read_csv(file)\n",
    "    log_df = log_df.merge(kmean300_df)\n",
    "\n",
    "    log_df_1 = log_df.loc[log_df['session_position']<=(log_df['session_length']/2)]\n",
    "    log_df_1['hour_of_day'] = log_df_1['hour_of_day'].astype('float')\n",
    "    log_df_1['premium'] = log_df_1['premium'].astype('bool')\n",
    "#     log_df_1['weekday'] = log_df_1['date'].astype('datetime64[ns]').dt.dayofweek\n",
    "    log_df_1 = log_df_1.drop(columns = ['date'])\n",
    "    log_df_1 = pd.get_dummies(log_df_1, columns=['hist_user_behavior_reason_end', 'hist_user_behavior_reason_start', 'context_type'], dtype = 'bool')\n",
    "#     log_df_1 = pd.get_dummies(log_df_1, columns=['hist_user_behavior_reason_end', 'hist_user_behavior_reason_start', 'context_type','weekday'], dtype = 'bool')\n",
    "    log_df_1 = log_df_1.merge(tf_df.drop(columns = ['time_signature','mode','key']))\n",
    "    \n",
    "                     \n",
    "    col_bool = log_df_1.columns[log_df_1.dtypes=='bool']\n",
    "    col_nonbool = log_df_1.columns[log_df_1.dtypes!='bool'].drop(['session_id','track_id_clean','clus'])\n",
    "    \n",
    "    # the non-convertable values will be set to 0\n",
    "    log_df_1[col_nonbool] = log_df_1[col_nonbool].apply(pd.to_numeric, errors='coerce', downcast = 'float').fillna(0).astype('float32')\n",
    "\n",
    "    # aggregate the track history where ['skip_2']==True\n",
    "    log_df_1_summary_skip2True = pd.concat([log_df_1.loc[log_df_1['skip_2']==True].groupby(['session_id'])[col_bool].agg(['mean']), \n",
    "                                            log_df_1.loc[log_df_1['skip_2']==True].groupby(['session_id'])[col_nonbool].agg(['mean', 'std', 'median'])],\n",
    "                                            axis = 1)\n",
    "    log_df_1_summary_skip2True.columns = log_df_1_summary_skip2True.columns.get_level_values(0)+'_sk2True_'+log_df_1_summary_skip2True.columns.get_level_values(1)\n",
    "    \n",
    "    # aggregate the track history where ['skip_2']==False\n",
    "    log_df_1_summary_skip2False = pd.concat([log_df_1.loc[log_df_1['skip_2']==False].groupby(['session_id'])[col_bool].agg(['mean']), \n",
    "                                             log_df_1.loc[log_df_1['skip_2']==False].groupby(['session_id'])[col_nonbool].agg(['mean', 'std', 'median'])],\n",
    "                                             axis = 1)\n",
    "    log_df_1_summary_skip2False.columns = log_df_1_summary_skip2False.columns.get_level_values(0)+'_sk2False_'+log_df_1_summary_skip2False.columns.get_level_values(1)\n",
    "    \n",
    "    \n",
    "    log_df_history = log_df_1[['session_id','track_id_clean','skip_2','clus']]\n",
    "\n",
    "\n",
    "    half_cut = log_df['session_length']/2\n",
    "\n",
    "    # need to at least include 2 trials, otherwise the log_df_1_summary will confound with all the tracks in the same session\n",
    "\n",
    "    #1st trial in the 2nd half\n",
    "    log_df_2_1 = log_df.loc[(log_df['session_position']>half_cut) & (log_df['session_position']<=half_cut+1)]\n",
    "    log_df_2_1 = log_df_2_1[['session_id','track_id_clean','skip_2','session_position','session_length','clus']]\n",
    "    log_df_2_1['weight'] = 1\n",
    "\n",
    "    #2nd trial in the 2nd half\n",
    "    log_df_2_2 = log_df.loc[(log_df['session_position']>half_cut+1) & (log_df['session_position']<=half_cut+2)]\n",
    "    log_df_2_2 = log_df_2_2[['session_id','track_id_clean','skip_2','session_position','session_length','clus']]\n",
    "    log_df_2_2['weight'] = 0.75\n",
    "\n",
    "    #3rd trial in the 2nd half\n",
    "    log_df_2_3 = log_df.loc[(log_df['session_position']>half_cut+2) & (log_df['session_position']<=half_cut+3)]\n",
    "    log_df_2_3 = log_df_2_3[['session_id','track_id_clean','skip_2','session_position','session_length','clus']]\n",
    "    log_df_2_3['weight'] = 0.62\n",
    "    \n",
    "    #4th trial in the 2nd half\n",
    "    log_df_2_4 = log_df.loc[(log_df['session_position']>half_cut+3) & (log_df['session_position']<=half_cut+4)]\n",
    "    log_df_2_4 = log_df_2_4[['session_id','track_id_clean','skip_2','session_position','session_length','clus']]\n",
    "    log_df_2_4['weight'] = 0.53\n",
    "    \n",
    "    #5th trial in the 2nd half\n",
    "    log_df_2_5 = log_df.loc[(log_df['session_position']>half_cut+4) & (log_df['session_position']<=half_cut+5)]\n",
    "    log_df_2_5 = log_df_2_5[['session_id','track_id_clean','skip_2','session_position','session_length','clus']]\n",
    "    log_df_2_5['weight'] = 0.47\n",
    "\n",
    "    #remaining trials in the 2nd half\n",
    "    log_df_2_6 = log_df.loc[(log_df['session_position']>half_cut+5)]\n",
    "    log_df_2_6 = log_df_2_6[['session_id','track_id_clean','skip_2','session_position','session_length','clus']]\n",
    "    log_df_2_6['weight'] = 0.35\n",
    "\n",
    "    log_df_2 = pd.concat([log_df_2_1,log_df_2_2,log_df_2_3,log_df_2_4,log_df_2_5,log_df_2_6])\n",
    "    log_df_2 = log_df_2.merge(log_df_1_summary_skip2True, on='session_id')\n",
    "    log_df_2 = log_df_2.merge(log_df_1_summary_skip2False, on='session_id')\n",
    "\n",
    "    sim_file_list = ['../models/SVD/all_tracks/similarity_ave20180917/k300_CanbDist.csv',\n",
    "                     '../models/SVD/all_tracks/similarity_ave20180917/k300_CosSim.csv',\n",
    "                     '../models/SVD/all_tracks/similarity_ave20180917/k300_LinCorr.csv',\n",
    "                     '../models/SVD/all_tracks/similarity_ave20180917/k300_ManhDist.csv',\n",
    "                     '../models/SVD/all_tracks/similarity_ave20180917/k300_HammDist.csv',\n",
    "                     '../models/SVD/all_tracks/similarity_ave20180917/k300_SpearCorr.csv',\n",
    "                     '../models/SVD/all_tracks/similarity_ave20180917/k300_KendCorr.csv',\n",
    "                     '../models/SVD/all_tracks/similarity_ave20180917/k300_ChebDist.csv',\n",
    "                     '../models/SVD/all_tracks/similarity_ave20180917/k300_BrayDist.csv']\n",
    "    score_name_list = ['CanbDist300', 'CosSim300','LinCorr300','ManhDist300','HammDist300','SpearCorr300','KendCorr300','ChebDist','BrayDist']\n",
    "\n",
    "    return get_sim(log_df_history, log_df_2, sim_file_list, score_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbde35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime per batch: 2313.71s\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from timeit import default_timer as timer #to see how long the computation will take\n",
    "\n",
    "nFile = 0\n",
    "batch_size = 10\n",
    "while nFile < len(file_list):\n",
    "    start = timer()\n",
    "    nFile += batch_size\n",
    "    df_lookup_list = []\n",
    "    for file in file_list[(nFile-batch_size):min(nFile, len(file_list))]:\n",
    "        df_lookup_list.append(prep_dfs(file, tf_df))\n",
    "\n",
    "    df_lookup = pd.concat(df_lookup_list)\n",
    "    df_lookup = df_lookup.merge(tf_df.drop(columns = ['key','time_signature','mode']))\n",
    "    \n",
    "    # check whether the column names match with the previous training set\n",
    "    if nFile>batch_size:\n",
    "        prev_feature_names = lgb.Booster(model_file='../models/SVD/LightGBM_BayesOpt_dec17/for20180918/boost_alltracks_1dayAhead.txt').feature_name()\n",
    "        if bool(set(prev_feature_names) - set(df_lookup.columns)): # if there are missing columns\n",
    "            df_lookup[list(set(prev_feature_names) - set(df_lookup.columns))] = 0 # add the missed columns with 0\n",
    "            \n",
    "        if bool(set(df_lookup.columns)- set(prev_feature_names)): # if there are extra columns\n",
    "            extra_cols = list(set(df_lookup.columns)- set(prev_feature_names) - set(['session_id','track_id_clean','skip_2','weight']))\n",
    "            df_lookup.drop(columns = extra_cols, inplace = True)\n",
    "    \n",
    "\n",
    "    dtrain = lgb.Dataset(df_lookup.drop(columns = ['session_id','track_id_clean','skip_2','weight']).astype('float32'), \n",
    "                     label=df_lookup['skip_2'],\n",
    "                     weight = df_lookup['weight'],\n",
    "                     free_raw_data=False) # https://lightgbm.readthedocs.io/en/latest/FAQ.html#error-messages-cannot-before-construct-dataset\n",
    "\n",
    "    \n",
    "    params = {'num_leaves': 25,\n",
    "              'learning_rate':0.15,\n",
    "              'metric': 'binary_error',\n",
    "              'num_iterations':851,\n",
    "              'bagging_fraction':0.8,\n",
    "              'bagging_freq':2,\n",
    "              'feature_fraction':0.73,\n",
    "              'min_gain_to_split':0.09,\n",
    "              'objective': 'binary',\n",
    "              'force_row_wise': True,\n",
    "              'num_threads': 5,\n",
    "              'verbosity': 0,\n",
    "              'tree_learner': 'voting_parallel'} #https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html\n",
    "    \n",
    "    if nFile == batch_size:\n",
    "        bst = lgb.train(params, dtrain)\n",
    "    else: # continue training on the previous model\n",
    "        bst = lgb.train(params, dtrain, init_model='../models/SVD/LightGBM_BayesOpt_dec17/for20180918/boost_alltracks_1dayAhead.txt')\n",
    "        \n",
    "    bst.save_model('../models/SVD/LightGBM_BayesOpt_dec17/for20180918/boost_alltracks_1dayAhead.txt')\n",
    "\n",
    "    print('Runtime per batch: %0.2fs' % (timer() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cdb949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
